{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3d7b189-d126-4f2b-b28c-1448fcddc439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradio in c:\\users\\19727\\anaconda3\\lib\\site-packages (4.39.0)\n",
      "Requirement already satisfied: requests in c:\\users\\19727\\anaconda3\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\19727\\anaconda3\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\19727\\anaconda3\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\19727\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\19727\\anaconda3\\lib\\site-packages (2.16.1)\n",
      "Requirement already satisfied: reportlab in c:\\users\\19727\\anaconda3\\lib\\site-packages (4.2.2)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from gradio) (23.2.1)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from gradio) (4.2.0)\n",
      "Requirement already satisfied: fastapi in c:\\users\\19727\\anaconda3\\lib\\site-packages (from gradio) (0.111.0)\n",
      "Requirement already satisfied: ffmpy in c:\\users\\19727\\anaconda3\\lib\\site-packages (from gradio) (0.3.2)\n",
      "Requirement already satisfied: gradio-client==1.1.1 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from gradio) (1.1.1)\n",
      "Requirement already satisfied: httpx>=0.24.1 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from gradio) (0.27.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from gradio) (0.20.3)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from gradio) (6.4.0)\n",
      "Requirement already satisfied: jinja2<4.0 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from gradio) (3.1.3)\n",
      "Requirement already satisfied: markupsafe~=2.0 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from gradio) (2.1.3)\n",
      "Requirement already satisfied: matplotlib~=3.0 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from gradio) (3.8.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from gradio) (1.26.4)\n",
      "Requirement already satisfied: orjson~=3.0 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from gradio) (3.10.6)\n",
      "Requirement already satisfied: packaging in c:\\users\\19727\\anaconda3\\lib\\site-packages (from gradio) (24.1)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from gradio) (2.1.4)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from gradio) (10.2.0)\n",
      "Requirement already satisfied: pydantic>=2.0 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from gradio) (2.8.2)\n",
      "Requirement already satisfied: pydub in c:\\users\\19727\\anaconda3\\lib\\site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from gradio) (0.0.9)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from gradio) (6.0.1)\n",
      "Requirement already satisfied: ruff>=0.2.2 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from gradio) (0.5.1)\n",
      "Requirement already satisfied: semantic-version~=2.0 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from gradio) (0.12.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from gradio) (0.12.3)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from gradio) (4.9.0)\n",
      "Requirement already satisfied: urllib3~=2.0 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from gradio) (2.0.7)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from gradio) (0.30.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\19727\\anaconda3\\lib\\site-packages (from gradio-client==1.1.1->gradio) (2023.10.0)\n",
      "Requirement already satisfied: websockets<12.0,>=10.0 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from gradio-client==1.1.1->gradio) (11.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: click in c:\\users\\19727\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\19727\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\19727\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.1 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from tensorflow) (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\19727\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.64.1)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: chardet in c:\\users\\19727\\anaconda3\\lib\\site-packages (from reportlab) (4.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (1.3.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\19727\\anaconda3\\lib\\site-packages (from httpx>=0.24.1->gradio) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\19727\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.19.3->gradio) (3.13.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from matplotlib~=3.0->gradio) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from matplotlib~=3.0->gradio) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from matplotlib~=3.0->gradio) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from matplotlib~=3.0->gradio) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from matplotlib~=3.0->gradio) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2023.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from pydantic>=2.0->gradio) (2.20.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\19727\\anaconda3\\lib\\site-packages (from tqdm->nltk) (0.4.6)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (13.3.5)\n",
      "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from fastapi->gradio) (0.37.2)\n",
      "Requirement already satisfied: fastapi-cli>=0.0.2 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from fastapi->gradio) (0.0.4)\n",
      "Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from fastapi->gradio) (5.4.0)\n",
      "Requirement already satisfied: email_validator>=2.0.0 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from fastapi->gradio) (2.2.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from email_validator>=2.0.0->fastapi->gradio) (2.6.1)\n",
      "Requirement already satisfied: namex in c:\\users\\19727\\anaconda3\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\19727\\anaconda3\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.11.0)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.15.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: httptools>=0.5.0 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.12.0->fastapi->gradio) (0.6.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.12.0->fastapi->gradio) (0.22.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\19727\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gradio requests python-dotenv beautifulsoup4 nltk tensorflow reportlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c4393ea-b75e-49dc-b178-ad8157df5064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\19727\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\19727\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Import necessary libraries\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "import gradio as gr\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "import numpy as np\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n",
    "from reportlab.lib.styles import getSampleStyleSheet\n",
    "from io import BytesIO\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"All libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df728405-9a01-4523-8c72-3143bf8d8070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardian API fetching function defined.\n"
     ]
    }
   ],
   "source": [
    "#Add the Guardian API Fetching code:\n",
    "API_KEY = os.getenv('GUARDIAN_API_KEY')\n",
    "BASE_URL = \"https://content.guardianapis.com/search\"\n",
    "\n",
    "def fetch_articles(start_date, end_date, section):\n",
    "    articles = []\n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        params = {\n",
    "            'api-key': API_KEY,\n",
    "            'section': section,\n",
    "            'from-date': current_date.strftime(\"%Y-%m-%d\"),\n",
    "            'to-date': (current_date + timedelta(days=1)).strftime(\"%Y-%m-%d\"),\n",
    "            'show-fields': 'bodyText',\n",
    "            'page-size': 50\n",
    "        }\n",
    "        response = requests.get(BASE_URL, params=params)\n",
    "        data = response.json()\n",
    "        articles.extend(data['response']['results'])\n",
    "        current_date += timedelta(days=1)\n",
    "        time.sleep(1)  # Rate limiting\n",
    "    return articles\n",
    "\n",
    "print(\"Guardian API fetching function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7df73c52-9155-4bc2-99d9-2ba0c200551d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text processing and summarization functions defined.\n"
     ]
    }
   ],
   "source": [
    "#Add the text processing and summarization functions:\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'http\\S+|www.\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "def summarize_text(text, num_sentences=3):\n",
    "    sentences = sent_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text.lower())\n",
    "    words = [word for word in words if word.isalnum() and word not in stop_words]\n",
    "    \n",
    "    freq = FreqDist(words)\n",
    "    \n",
    "    sentence_scores = {}\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for word in word_tokenize(sentence.lower()):\n",
    "            if word in freq:\n",
    "                if i in sentence_scores:\n",
    "                    sentence_scores[i] += freq[word]\n",
    "                else:\n",
    "                    sentence_scores[i] = freq[word]\n",
    "    \n",
    "    top_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:num_sentences]\n",
    "    summary = ' '.join([sentences[i] for i in sorted(top_sentences)])\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def get_top_snippets(articles, n=3):\n",
    "    def get_article_date(article):\n",
    "        return article.get('webPublicationDate', '')\n",
    "    \n",
    "    sorted_articles = sorted(articles, key=get_article_date, reverse=True)\n",
    "    snippets = []\n",
    "    for article in sorted_articles[:n]:\n",
    "        title = article.get('webTitle', 'No title')\n",
    "        body = article.get('fields', {}).get('bodyText', 'No description')\n",
    "        snippet = f\"{title}: {body[:100]}...\"\n",
    "        snippets.append(snippet)\n",
    "    return snippets\n",
    "\n",
    "print(\"Text processing and summarization functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f28d24f-1a00-4fea-bdad-4008293ec821",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to synchronously open file (unable to open file: name = 'guardian_article_classifier_final.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load the model and necessary components\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m load_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mguardian_article_classifier_final.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenizer_final.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[0;32m      4\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(handle)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_api.py:183\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    177\u001b[0m         filepath,\n\u001b[0;32m    178\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m    179\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[0;32m    180\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[0;32m    181\u001b[0m     )\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m--> 183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m legacy_h5_format\u001b[38;5;241m.\u001b[39mload_model_from_hdf5(\n\u001b[0;32m    184\u001b[0m         filepath, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects, \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m\n\u001b[0;32m    185\u001b[0m     )\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    188\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    189\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    190\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    191\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\legacy\\saving\\legacy_h5_format.py:116\u001b[0m, in \u001b[0;36mload_model_from_hdf5\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    114\u001b[0m opened_new_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath, h5py\u001b[38;5;241m.\u001b[39mFile)\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opened_new_file:\n\u001b[1;32m--> 116\u001b[0m     f \u001b[38;5;241m=\u001b[39m h5py\u001b[38;5;241m.\u001b[39mFile(filepath, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     f \u001b[38;5;241m=\u001b[39m filepath\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\h5py\\_hl\\files.py:562\u001b[0m, in \u001b[0;36mFile.__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[0;32m    553\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[0;32m    554\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[0;32m    555\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[0;32m    556\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[0;32m    557\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[0;32m    558\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    559\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[0;32m    560\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[0;32m    561\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[1;32m--> 562\u001b[0m     fid \u001b[38;5;241m=\u001b[39m make_fid(name, mode, userblock_size, fapl, fcpl, swmr\u001b[38;5;241m=\u001b[39mswmr)\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    565\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\h5py\\_hl\\files.py:235\u001b[0m, in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[0;32m    234\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[1;32m--> 235\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, flags, fapl\u001b[38;5;241m=\u001b[39mfapl)\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    237\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[1;32mh5py\\\\_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\\\_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\\\h5f.pyx:102\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to synchronously open file (unable to open file: name = 'guardian_article_classifier_final.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "# Load the guardian model and define the predication function:\n",
    "model = load_model('guardian_article_classifier_final.h5')\n",
    "with open('tokenizer_final.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "with open('label_encoder_final.pickle', 'rb') as handle:\n",
    "    label_encoder = pickle.load(handle)\n",
    "\n",
    "max_len = 200  # Make sure this matches the value used during training\n",
    "\n",
    "def predict_article_category(title, body, model, tokenizer, label_encoder):\n",
    "    text = f\"{title} {body}\"\n",
    "    processed_text = preprocess_text(text)\n",
    "    text_seq = tokenizer.texts_to_sequences([processed_text])\n",
    "    text_pad = pad_sequences(text_seq, maxlen=max_len)\n",
    "    prediction = model.predict(text_pad)\n",
    "    predicted_class_index = np.argmax(prediction, axis=1)[0]\n",
    "    predicted_class = label_encoder.classes_[predicted_class_index]\n",
    "    return predicted_class\n",
    "\n",
    "print(\"Guardian model loaded and prediction function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac17809-f7f8-4717-836b-c45052d608a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the main news summary function:\n",
    "def get_news_summary(topic, language='en', sort='newest', limit=10):\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=7)\n",
    "    sections = ['politics', 'business', 'technology', 'sport', 'culture']\n",
    "    all_articles = []\n",
    "    \n",
    "    for section in sections:\n",
    "        all_articles.extend(fetch_articles(start_date, end_date, section))\n",
    "    \n",
    "    # Filter articles based on the topic\n",
    "    filtered_articles = [article for article in all_articles if topic.lower() in article['webTitle'].lower()]\n",
    "    \n",
    "    # Sort articles\n",
    "    if sort == 'newest':\n",
    "        filtered_articles.sort(key=lambda x: x['webPublicationDate'], reverse=True)\n",
    "    elif sort == 'oldest':\n",
    "        filtered_articles.sort(key=lambda x: x['webPublicationDate'])\n",
    "    \n",
    "    # Limit the number of articles\n",
    "    filtered_articles = filtered_articles[:limit]\n",
    "    \n",
    "    # Classify articles\n",
    "    classified_articles = []\n",
    "    for article in filtered_articles:\n",
    "        title = article.get('webTitle', '')\n",
    "        body = article.get('fields', {}).get('bodyText', '')\n",
    "        category = predict_article_category(title, body, model, tokenizer, label_encoder)\n",
    "        article['category'] = category\n",
    "        classified_articles.append(article)\n",
    "\n",
    "    # Group articles by category\n",
    "    categorized_articles = {}\n",
    "    for article in classified_articles:\n",
    "        category = article['category']\n",
    "        if category not in categorized_articles:\n",
    "            categorized_articles[category] = []\n",
    "        categorized_articles[category].append(article)\n",
    "\n",
    "    # Generate summary for each category\n",
    "    output = f\"Summary of recent news on '{topic}':\\n\\n\"\n",
    "    for category, cat_articles in categorized_articles.items():\n",
    "        output += f\"{category.upper()}:\\n\"\n",
    "        cat_content = \" \".join([art.get('fields', {}).get('bodyText', '') for art in cat_articles])\n",
    "        cat_summary = summarize_text(cat_content, num_sentences=2)\n",
    "        output += f\"{cat_summary}\\n\\n\"\n",
    "\n",
    "    # Get top snippets and sources\n",
    "    top_snippets = get_top_snippets(filtered_articles)\n",
    "    output += \"Top Articles:\\n\"\n",
    "    for i, snippet in enumerate(top_snippets, 1):\n",
    "        output += f\"{i}. {snippet}\\n\"\n",
    "\n",
    "    sources = set(article.get('sectionName', 'Unknown') for article in filtered_articles)\n",
    "    output += f\"\\nSources: {', '.join(sources)}\"\n",
    "\n",
    "    return output\n",
    "\n",
    "print(\"Main news summary function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6066adb-b973-4894-9ddb-1ace690f3314",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the PDF generation function:\n",
    "def generate_pdf(content):\n",
    "    buffer = BytesIO()\n",
    "    doc = SimpleDocTemplate(buffer, pagesize=letter)\n",
    "    styles = getSampleStyleSheet()\n",
    "    story = []\n",
    "\n",
    "    # Add title\n",
    "    story.append(Paragraph(\"News Summary\", styles['Title']))\n",
    "    story.append(Spacer(1, 12))\n",
    "\n",
    "    # Add content\n",
    "    for line in content.split('\\n'):\n",
    "        if line.strip():\n",
    "            story.append(Paragraph(line, styles['BodyText']))\n",
    "            story.append(Spacer(1, 6))\n",
    "\n",
    "    doc.build(story)\n",
    "    buffer.seek(0)\n",
    "    return buffer\n",
    "\n",
    "print(\"PDF generation function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e443ee53-f308-4931-a8a6-59572d61aa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the Gradio interface:\n",
    "def get_news_summary_with_pdf(topic, language='en', sort='newest', limit=10):\n",
    "    text_output = get_news_summary(topic, language, sort, limit)\n",
    "    pdf_buffer = generate_pdf(text_output)\n",
    "    return text_output, pdf_buffer\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=get_news_summary_with_pdf,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Enter the topic you want a summary for:\"),\n",
    "        gr.Dropdown(choices=[\"en\"], label=\"Language\", value=\"en\"),\n",
    "        gr.Dropdown(choices=[\"newest\", \"oldest\"], label=\"Sort By\", value=\"newest\"),\n",
    "        gr.Slider(minimum=1, maximum=25, step=1, label=\"Number of Articles\", value=10)\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"Summary and Sources\"),\n",
    "        gr.File(label=\"Download PDF\")\n",
    "    ],\n",
    "    title=\"Neural Newsroom\",\n",
    "    description=\"Get a summary of the latest news on a given topic from The Guardian, classified by our model. You can also download the summary as a PDF.\",\n",
    "    examples=[[\"climate change\"], [\"artificial intelligence\"], [\"global economy\"]],\n",
    ")\n",
    "\n",
    "iface.launch()\n",
    "\n",
    "print(\"Gradio interface launched. You can now interact with the application.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
