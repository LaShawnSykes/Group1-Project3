{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1e23931-225b-4ed6-ad0f-a5d02ca3e976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported and environment variables loaded.\n",
      "TensorFlow version: 2.17.0\n",
      "NumPy version: 1.26.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\19727\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\19727\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Import libraries and Setup\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "import gradio as gr\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense\n",
    "import pickle\n",
    "import numpy as np\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n",
    "from reportlab.lib.styles import getSampleStyleSheet\n",
    "from io import BytesIO\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"Libraries imported and environment variables loaded.\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07d9f34b-be60-44f4-b8e1-59a8073c0d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load label encoder\n",
      "Loaded label encoder from label_encoder_final.pickle\n",
      "Label encoder loaded successfully\n",
      "Creating new tokenizer\n",
      "New tokenizer created successfully\n",
      "Building the model\n",
      "Model built successfully\n",
      "Components loaded/created successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\19727\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.3.2 when using version 1.4.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\19727\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Load Components and Build Model\n",
    "def load_label_encoder():\n",
    "    label_encoder_file = 'label_encoder_final.pickle'\n",
    "    with open(label_encoder_file, 'rb') as handle:\n",
    "        label_encoder = pickle.load(handle)\n",
    "    print(f\"Loaded label encoder from {label_encoder_file}\")\n",
    "    return label_encoder\n",
    "\n",
    "# Try to load the components and build the model\n",
    "try:\n",
    "    print(\"Attempting to load label encoder\")\n",
    "    label_encoder = load_label_encoder()\n",
    "    print(\"Label encoder loaded successfully\")\n",
    "    \n",
    "    max_len = 200\n",
    "    max_words = 10000\n",
    "    \n",
    "    print(\"Creating new tokenizer\")\n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    print(\"New tokenizer created successfully\")\n",
    "    \n",
    "    print(\"Building the model\")\n",
    "    model = Sequential([\n",
    "        Embedding(max_words, 16, input_length=max_len),\n",
    "        GlobalAveragePooling1D(),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(len(label_encoder.classes_), activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    print(\"Model built successfully\")\n",
    "    \n",
    "    print(\"Components loaded/created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Please ensure the necessary files are in the current directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c0b4aaf-272e-443c-915f-a5e37ec6de2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined.\n"
     ]
    }
   ],
   "source": [
    "#Helper Functions\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'http\\S+|www.\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "def summarize_text(text, num_sentences=3):\n",
    "    sentences = sent_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text.lower())\n",
    "    words = [word for word in words if word.isalnum() and word not in stop_words]\n",
    "    \n",
    "    freq = FreqDist(words)\n",
    "    \n",
    "    sentence_scores = {}\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for word in word_tokenize(sentence.lower()):\n",
    "            if word in freq:\n",
    "                if i in sentence_scores:\n",
    "                    sentence_scores[i] += freq[word]\n",
    "                else:\n",
    "                    sentence_scores[i] = freq[word]\n",
    "    \n",
    "    top_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:num_sentences]\n",
    "    summary = ' '.join([sentences[i] for i in sorted(top_sentences)])\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def get_top_snippets(articles, n=3):\n",
    "    def get_article_date(article):\n",
    "        return article.get('webPublicationDate', '')\n",
    "    \n",
    "    sorted_articles = sorted(articles, key=get_article_date, reverse=True)\n",
    "    snippets = []\n",
    "    for article in sorted_articles[:n]:\n",
    "        title = article.get('webTitle', 'No title')\n",
    "        body = article.get('fields', {}).get('bodyText', 'No description')\n",
    "        snippet = f\"{title}: {body[:100]}...\"\n",
    "        snippets.append(snippet)\n",
    "    return snippets\n",
    "\n",
    "def predict_article_category(title, body):\n",
    "    text = f\"{title} {body}\"\n",
    "    tokenizer.fit_on_texts([text])\n",
    "    text_seq = tokenizer.texts_to_sequences([text])\n",
    "    text_pad = pad_sequences(text_seq, maxlen=max_len)\n",
    "    \n",
    "    prediction = model.predict(text_pad)\n",
    "    predicted_class_index = np.argmax(prediction, axis=1)[0]\n",
    "    predicted_class = label_encoder.classes_[predicted_class_index]\n",
    "    return predicted_class\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "335f4de4-20f7-439a-a9ec-cc3c1607ec11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardian API fetching function defined.\n"
     ]
    }
   ],
   "source": [
    "#Guardian API Fetching Function\n",
    "API_KEY = os.getenv('GUARDIAN_API_KEY')\n",
    "BASE_URL = \"https://content.guardianapis.com/search\"\n",
    "\n",
    "def fetch_articles(start_date, end_date, section):\n",
    "    articles = []\n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        params = {\n",
    "            'api-key': API_KEY,\n",
    "            'section': section,\n",
    "            'from-date': current_date.strftime(\"%Y-%m-%d\"),\n",
    "            'to-date': (current_date + timedelta(days=1)).strftime(\"%Y-%m-%d\"),\n",
    "            'show-fields': 'bodyText',\n",
    "            'page-size': 50\n",
    "        }\n",
    "        response = requests.get(BASE_URL, params=params)\n",
    "        data = response.json()\n",
    "        articles.extend(data['response']['results'])\n",
    "        current_date += timedelta(days=1)\n",
    "        time.sleep(1)  # Rate limiting\n",
    "    return articles\n",
    "\n",
    "print(\"Guardian API fetching function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1374cb34-f2ca-40f1-bf9e-68486878e487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main news summary function defined.\n"
     ]
    }
   ],
   "source": [
    "#Main News Summary Functions\n",
    "def get_news_summary(topic, language='en', sort='newest', limit=10):\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=7)\n",
    "    sections = ['politics', 'business', 'technology', 'sport', 'culture']\n",
    "    all_articles = []\n",
    "    \n",
    "    for section in sections:\n",
    "        all_articles.extend(fetch_articles(start_date, end_date, section))\n",
    "    \n",
    "    # Filter articles based on the topic\n",
    "    filtered_articles = [article for article in all_articles if topic.lower() in article['webTitle'].lower()]\n",
    "    \n",
    "    # Sort articles\n",
    "    if sort == 'newest':\n",
    "        filtered_articles.sort(key=lambda x: x['webPublicationDate'], reverse=True)\n",
    "    elif sort == 'oldest':\n",
    "        filtered_articles.sort(key=lambda x: x['webPublicationDate'])\n",
    "    \n",
    "    # Limit the number of articles\n",
    "    filtered_articles = filtered_articles[:limit]\n",
    "    \n",
    "    # Classify articles\n",
    "    classified_articles = []\n",
    "    for article in filtered_articles:\n",
    "        title = article.get('webTitle', '')\n",
    "        body = article.get('fields', {}).get('bodyText', '')\n",
    "        category = predict_article_category(title, body)\n",
    "        article['category'] = category\n",
    "        classified_articles.append(article)\n",
    "\n",
    "    # Group articles by category\n",
    "    categorized_articles = {}\n",
    "    for article in classified_articles:\n",
    "        category = article['category']\n",
    "        if category not in categorized_articles:\n",
    "            categorized_articles[category] = []\n",
    "        categorized_articles[category].append(article)\n",
    "\n",
    "    # Generate summary for each category\n",
    "    output = f\"Summary of recent news on '{topic}':\\n\\n\"\n",
    "    for category, cat_articles in categorized_articles.items():\n",
    "        output += f\"{category.upper()}:\\n\"\n",
    "        cat_content = \" \".join([art.get('fields', {}).get('bodyText', '') for art in cat_articles])\n",
    "        cat_summary = summarize_text(cat_content, num_sentences=2)\n",
    "        output += f\"{cat_summary}\\n\\n\"\n",
    "\n",
    "    # Get top snippets and sources\n",
    "    top_snippets = get_top_snippets(filtered_articles)\n",
    "    output += \"Top Articles:\\n\"\n",
    "    for i, snippet in enumerate(top_snippets, 1):\n",
    "        output += f\"{i}. {snippet}\\n\"\n",
    "\n",
    "    sources = set(article.get('sectionName', 'Unknown') for article in filtered_articles)\n",
    "    output += f\"\\nSources: {', '.join(sources)}\"\n",
    "\n",
    "    return output\n",
    "\n",
    "print(\"Main news summary function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21c166d1-9af9-4670-bb24-abd3bb274391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF generation function defined.\n"
     ]
    }
   ],
   "source": [
    "#PDF Generation Function\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "def generate_pdf(content):\n",
    "    buffer = BytesIO()\n",
    "    doc = SimpleDocTemplate(buffer, pagesize=letter)\n",
    "    styles = getSampleStyleSheet()\n",
    "    story = []\n",
    "\n",
    "    # Add title\n",
    "    story.append(Paragraph(\"News Summary\", styles['Title']))\n",
    "    story.append(Spacer(1, 12))\n",
    "\n",
    "    # Add content\n",
    "    for line in content.split('\\n'):\n",
    "        if line.strip():\n",
    "            story.append(Paragraph(line, styles['BodyText']))\n",
    "            story.append(Spacer(1, 6))\n",
    "\n",
    "    doc.build(story)\n",
    "    buffer.seek(0)\n",
    "    \n",
    "    # Create a temporary file\n",
    "    temp_pdf = tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\")\n",
    "    temp_pdf.write(buffer.getvalue())\n",
    "    temp_pdf.close()\n",
    "    \n",
    "    return temp_pdf.name\n",
    "\n",
    "print(\"PDF generation function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f40cec97-ea26-4bb5-9587-86142cbd2d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradio interface set up. Ready to launch.\n"
     ]
    }
   ],
   "source": [
    "#Gradio Interface Setup\n",
    "def get_news_summary_with_pdf(topic, language='en', sort='newest', limit=10):\n",
    "    text_output = get_news_summary(topic, language, sort, limit)\n",
    "    pdf_path = generate_pdf(text_output)\n",
    "    return text_output, pdf_path\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=get_news_summary_with_pdf,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Enter the topic you want a summary for:\"),\n",
    "        gr.Dropdown(choices=[\"en\"], label=\"Language\", value=\"en\"),\n",
    "        gr.Dropdown(choices=[\"newest\", \"oldest\"], label=\"Sort By\", value=\"newest\"),\n",
    "        gr.Slider(minimum=1, maximum=25, step=1, label=\"Number of Articles\", value=10)\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"Summary and Sources\"),\n",
    "        gr.File(label=\"Download PDF\")\n",
    "    ],\n",
    "    title=\"Neural Newsroom\",\n",
    "    description=\"Get a summary of the latest news on a given topic from The Guardian, classified by our model. You can also download the summary as a PDF.\",\n",
    "    examples=[[\"climate change\"], [\"artificial intelligence\"], [\"global economy\"]],\n",
    ")\n",
    "\n",
    "print(\"Gradio interface set up. Ready to launch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e2a6850-ab05-48b0-a615-39bbee482dd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n"
     ]
    }
   ],
   "source": [
    "iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf4fd5d-3553-4dee-9460-e3764f3b989d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
