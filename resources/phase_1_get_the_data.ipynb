{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I understand you need to import news articles from 10 different sources using APIs, preprocess them, and combine them into a single dataframe for an unsupervised learning model. I'll outline a step-by-step approach to accomplish this task:\n",
    "\n",
    "1. Choose and set up APIs:\n",
    "Select 10 APIs from the list provided. For this example, let's assume we're using:\n",
    "- NewsAPI\n",
    "- Bing News Search API\n",
    "- New York Times API\n",
    "- The Guardian API\n",
    "- GDELT Project\n",
    "- Currents API\n",
    "- Event Registry\n",
    "- Reuters News API\n",
    "- MediaStack API\n",
    "- ContextualWeb News API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Import necessary libraries:\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Define a function to fetch articles from each API:\n",
    "\n",
    "def fetch_articles(api_url, api_key, params):\n",
    "    headers = {'Authorization': f'Bearer {api_key}'}\n",
    "    response = requests.get(api_url, headers=headers, params=params)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create a function to preprocess the text:\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Join the tokens back into a string\n",
    "    processed_text = ' '.join(tokens)\n",
    "    \n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "apis = [\n",
    "    {\n",
    "        'name': 'NewsAPI',\n",
    "        'url': 'https://newsapi.org/v2/top-headlines',\n",
    "        'key': 'your_newsapi_key_here',\n",
    "        'params': {'country': 'us', 'pageSize': 100}\n",
    "    },\n",
    "    {\n",
    "        'name': 'Bing News Search',\n",
    "        'url': 'https://api.bing.microsoft.com/v7.0/news/search',\n",
    "        'key': 'your_bing_api_key_here',\n",
    "        'params': {'q': 'top stories', 'count': 100}\n",
    "    },\n",
    "    # Add the remaining APIs here...\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m articles \u001b[38;5;241m=\u001b[39m fetch_articles(api[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'\u001b[39m], api[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkey\u001b[39m\u001b[38;5;124m'\u001b[39m], api[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m articles:\n\u001b[0;32m      8\u001b[0m     processed_article \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m'\u001b[39m: api[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m---> 10\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m: preprocess_text(\u001b[43marticle\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtitle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m),\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m: preprocess_text(article[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpublication_date\u001b[39m\u001b[38;5;124m'\u001b[39m: article[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpublication_date\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     13\u001b[0m     }\n\u001b[0;32m     14\u001b[0m     all_articles\u001b[38;5;241m.\u001b[39mappend(processed_article)\n",
      "\u001b[1;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "# 5. Fetch and preprocess articles from each API:\n",
    "\n",
    "all_articles = []\n",
    "\n",
    "for api in apis:\n",
    "    articles = fetch_articles(api['url'], api['key'], api['params'])\n",
    "    for article in articles:\n",
    "        processed_article = {\n",
    "            'source': api['name'],\n",
    "            'title': preprocess_text(article['title']),\n",
    "            'content': preprocess_text(article['content']),\n",
    "            'publication_date': article['publication_date']\n",
    "        }\n",
    "        all_articles.append(processed_article)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Combine all articles into a single dataframe:\n",
    "df = pd.DataFrame(all_articles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Handle missing values and duplicates:\n",
    "\n",
    "# Remove rows with missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Remove duplicate articles based on content\n",
    "df = df.drop_duplicates(subset='content')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Feature engineering (if needed):\n",
    "\n",
    "# Example: Create a 'text_length' feature\n",
    "df['text_length'] = df['content'].apply(len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Prepare the data for the unsupervised learning model:\n",
    "\n",
    "# Combine title and content for text analysis\n",
    "df['full_text'] = df['title'] + ' ' + df['content']\n",
    "\n",
    "# Use TF-IDF vectorization\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "tfidf_matrix = vectorizer.fit_transform(df['full_text'])\n",
    "\n",
    "# Convert the TF-IDF matrix to a dataframe\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Combine with other features if needed\n",
    "final_df = pd.concat([df[['source', 'publication_date', 'text_length']], tfidf_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have a preprocessed dataframe (`final_df`) ready for your unsupervised learning model. This dataframe contains the processed text data along with additional features like the source, publication date, and text length.\n",
    "\n",
    "Remember to handle API authentication properly and respect rate limits for each API. Also, make sure you have the necessary permissions and comply with the terms of service for each API you use.\n",
    "\n",
    "Would you like me to explain or elaborate on any part of this process?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
